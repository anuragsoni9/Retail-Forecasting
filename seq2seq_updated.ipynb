{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers.advanced_activations import PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.8\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from numpy import array_equal\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns train, inference_encoder and inference_decoder models\n",
    "def define_models(n_input, n_output, n_units):\n",
    "    \n",
    "\t# define training encoder\n",
    "\tencoder_inputs = Input(shape=(None, n_input))\n",
    "\tencoder = LSTM(n_units, return_state=True)\n",
    "\tencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\tencoder_states = [state_h, state_c]\n",
    "    \n",
    "\t# define training decoder\n",
    "\tdecoder_inputs = Input(shape=(None, n_output))\n",
    "\tdecoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)\n",
    "\tdecoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "\tdecoder_dense = Dense(n_output, activation='relu')\n",
    "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "\tmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "    \n",
    "\t# define inference encoder\n",
    "\tencoder_model = Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "\t# define inference decoder\n",
    "\tdecoder_state_input_h = Input(shape=(n_units,))\n",
    "\tdecoder_state_input_c = Input(shape=(n_units,))\n",
    "\tdecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\tdecoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "\tdecoder_states = [state_h, state_c]\n",
    "\tdecoder_outputs = decoder_dense(decoder_outputs)\n",
    "\tdecoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "    \n",
    "\t# return all models\n",
    "\treturn model, encoder_model, decoder_model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model on actual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to fit on numerical data. The seq to seq method is generally for one-hot encoded variables but here we use it on numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    'D:/data mining/infor project/train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "    float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 66458909)  # 2016-01-01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\n",
    "    \"D:/data mining/infor project/test.csv\", usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "\n",
    "items = pd.read_csv(\n",
    "    \"D:/data mining/infor project/items.csv\",\n",
    ").set_index(\"item_nbr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2017,1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unstacking the date varialbe (pivoting the dates into columns)\n",
    "df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017.columns = df_2017.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_items = pd.DataFrame(index=df_2017.index)\n",
    "test_ids = df_test[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeating and reordering the items table to align with the df_2017 tables \n",
    "items = items.reindex( stores_items.index.get_level_values(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training, validation, and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = df_2017.iloc[:,145:195].as_matrix()\n",
    "X1_train = X1_train.reshape((X1_train.shape[0],X1_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_2017.iloc[:,195:211].as_matrix()\n",
    "y_train = y_train.reshape((y_train.shape[0],y_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train = np.insert(y_train[:,:-1],0,[-1],axis =1)\n",
    "X2_train = X2_train.reshape((X2_train.shape[0],X2_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_val = df_2017.iloc[:,161:211].as_matrix()\n",
    "X1_val = X1_val.reshape((X1_val.shape[0],X1_val.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = df_2017.iloc[:,211:].as_matrix()\n",
    "y_val = y_val.reshape((y_val.shape[0],y_val.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_val = np.insert(y_val[:,:-1],0,[-1],axis =1)\n",
    "X2_val = X2_val.reshape((X2_val.shape[0],X2_val.shape[1],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_test = df_2017.iloc[:,177:].as_matrix()\n",
    "X1_test = X1_test.reshape((X1_test.shape[0],X1_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, infenc, infdec = define_models(1, 1, 128)\n",
    "train.compile(optimizer='adam', loss='mse', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 167515 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "167515/167515 [==============================] - 590s - loss: 0.3739 - mean_squared_error: 0.3528 - val_loss: 0.3463 - val_mean_squared_error: 0.3463\n",
      "Epoch 2/5\n",
      "167515/167515 [==============================] - 579s - loss: 0.3553 - mean_squared_error: 0.3353 - val_loss: 0.3456 - val_mean_squared_error: 0.3456\n",
      "Epoch 3/5\n",
      "167515/167515 [==============================] - 573s - loss: 0.3519 - mean_squared_error: 0.3322 - val_loss: 0.3399 - val_mean_squared_error: 0.3399\n",
      "Epoch 4/5\n",
      "167515/167515 [==============================] - 574s - loss: 0.3504 - mean_squared_error: 0.3308 - val_loss: 0.3419 - val_mean_squared_error: 0.3419\n",
      "Epoch 5/5\n",
      "167515/167515 [==============================] - 577s - loss: 0.3491 - mean_squared_error: 0.3295 - val_loss: 0.3425 - val_mean_squared_error: 0.3425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22f821027f0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights=np.array( pd.concat([items[\"perishable\"]] * 1) * 0.25 + 1 )\n",
    "train.fit([X1_train, X2_train], y_train, epochs=5,sample_weight=sample_weights,\n",
    "          validation_data=([X1_val, X2_val], y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "state = infenc.predict(X1_val)\n",
    "# start of sequence input\n",
    "target_seq = array([-1 for _ in range(167515)]).reshape(167515, 1, 1)\n",
    "# collect predictions\n",
    "output = list()\n",
    "for t in range(16):\n",
    "    # predict next char\n",
    "    yhat, h, c = infdec.predict([target_seq] + state)\n",
    "    # store prediction\n",
    "    output.append(yhat[:,0,0])\n",
    "    # update state\n",
    "    state = [h, c]\n",
    "    # update target sequence\n",
    "    target_seq = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_val =  array(output).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted validation mse: 0.405532251137\n",
      "Full validation mse:        0.406096756002\n"
     ]
    }
   ],
   "source": [
    "weights=pd.concat([items[\"perishable\"]]) * 0.25 + 1\n",
    "print(\"Unweighted validation mse:\", mean_squared_error(\n",
    "    y_val.reshape((167515,16)), y_hat_val))\n",
    "\n",
    "print(\"Full validation mse:       \", mean_squared_error(\n",
    "     y_val.reshape((167515,16)), y_hat_val, sample_weight=weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode\n",
    "state = infenc.predict(X1_test)\n",
    "# start of sequence input\n",
    "target_seq = array([-1 for _ in range(167515)]).reshape(167515, 1, 1)\n",
    "# collect predictions\n",
    "output = list()\n",
    "for t in range(16):\n",
    "    # predict next char\n",
    "    yhat, h, c = infdec.predict([target_seq] + state)\n",
    "    # store prediction\n",
    "    output.append(yhat[:,0,0])\n",
    "    # update state\n",
    "    state = [h, c]\n",
    "    # update target sequence\n",
    "    target_seq = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  array(output).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=stores_items.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_ids.join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission.to_csv('seq2seq1.csv', float_format='%.4f', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions for meta model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_meta = []\n",
    "y_meta = []\n",
    "for i in range(100,131):\n",
    "    X1_meta_tmp = df_2017.iloc[:,i:(i+50)].as_matrix()\n",
    "    X1_meta_tmp = X1_meta_tmp.reshape((X1_meta_tmp.shape[0],X1_meta_tmp.shape[1],1))\n",
    "    X1_meta.append(X1_meta_tmp)\n",
    "    y_meta_tmp = df_2017.iloc[:,(i+50):(i+66)].as_matrix()\n",
    "    y_meta_tmp = y_meta_tmp.reshape((y_meta_tmp.shape[0],y_meta_tmp.shape[1],1))\n",
    "    y_meta.append(y_meta_tmp)\n",
    "X1_meta = np.concatenate(X1_meta,axis =0)\n",
    "y_meta = np.concatenate(y_meta,axis =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict for meta model\n",
    "# encode\n",
    "state = infenc.predict(X1_meta)\n",
    "# start of sequence input\n",
    "target_seq = array([-1 for _ in range(X1_meta.shape[0])]).reshape(X1_meta.shape[0], 1, 1)\n",
    "# collect predictions\n",
    "output = list()\n",
    "for t in range(16):\n",
    "    # predict next char\n",
    "    yhat, h, c = infdec.predict([target_seq] + state)\n",
    "    # store prediction\n",
    "    output.append(yhat[:,0,0])\n",
    "    # update state\n",
    "    state = [h, c]\n",
    "    # update target sequence\n",
    "    target_seq = yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_meta =  array(output).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted validation mse: 0.411570710818\n",
      "Full validation mse:        0.412970885846\n"
     ]
    }
   ],
   "source": [
    "weights=pd.concat([items[\"perishable\"]]*31) * 0.25 + 1\n",
    "print(\"Unweighted validation mse:\", mean_squared_error(\n",
    "    y_meta.reshape((167515*31,16)), y_hat_meta))\n",
    "\n",
    "print(\"Full validation mse:       \", mean_squared_error(\n",
    "     y_meta.reshape((167515*31,16)), y_hat_meta, sample_weight=weights))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
