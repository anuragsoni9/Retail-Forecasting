{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Dense, Dropout, Activation, Reshape\n",
    "from keras.layers import LSTM\n",
    "from keras import callbacks\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "from keras.layers import Merge\n",
    "from keras.layers.embeddings import Embedding\n",
    "import h5py\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Input\n",
    "from keras.models import Model, Sequential\n",
    "import copy\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\n",
    "    'D:/data mining/infor project/train.csv', usecols=[1, 2, 3, 4, 5],\n",
    "    dtype={'onpromotion': bool},\n",
    "    converters={'unit_sales': lambda u: np.log1p(\n",
    "        float(u)) if float(u) > 0 else 0},\n",
    "    parse_dates=[\"date\"],\n",
    "    skiprows=range(1, 66458909)  # 2016-01-01\n",
    ")\n",
    "\n",
    "df_test = pd.read_csv(\n",
    "    \"D:/data mining/infor project/test.csv\", usecols=[0, 1, 2, 3, 4],\n",
    "    dtype={'onpromotion': bool},\n",
    "    parse_dates=[\"date\"]  # , date_parser=parser\n",
    ").set_index(\n",
    "    ['store_nbr', 'item_nbr', 'date']\n",
    ")\n",
    "\n",
    "items = pd.read_csv(\n",
    "    \"D:/data mining/infor project/items.csv\",\n",
    ").set_index(\"item_nbr\")\n",
    "\n",
    "stores = pd.read_csv(\n",
    "    \"D:/data mining/infor project/stores.csv\",\n",
    ").set_index(\"store_nbr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "items['family'] = le.fit_transform(items['family'].values)\n",
    "items['class'] = le.fit_transform(items['class'].values)\n",
    "items['perishable'] = le.fit_transform(items['perishable'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores['city'] = le.fit_transform(stores['city'].values)\n",
    "stores['state'] = le.fit_transform(stores['state'].values)\n",
    "stores['type'] = le.fit_transform(stores['type'].values)\n",
    "stores['cluster'] = le.fit_transform(stores['cluster'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df_train.loc[df_train.date>=pd.datetime(2017,1,1)]\n",
    "del df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_2017_train = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(\n",
    "        level=-1).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_2017_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_2017_test.columns = promo_2017_test.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\n",
    "del promo_2017_test, promo_2017_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017 = df_2017.set_index(\n",
    "    [\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(\n",
    "        level=-1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2017.columns = df_2017.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.reindex(df_2017.index.get_level_values(1))\n",
    "stores = stores.reindex(df_2017.index.get_level_values(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the datasets for training and testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timespan(df, dt, minus, periods, freq='D'):\n",
    "    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(t2017, is_train=True):\n",
    "    X = pd.DataFrame({\n",
    "        \"day_1_2017\": get_timespan(df_2017, t2017, 1, 1).values.ravel(),\n",
    "        \"mean_3_2017\": get_timespan(df_2017, t2017, 3, 3).mean(axis=1).values,\n",
    "        \"mean_7_2017\": get_timespan(df_2017, t2017, 7, 7).mean(axis=1).values,\n",
    "        \"mean_14_2017\": get_timespan(df_2017, t2017, 14, 14).mean(axis=1).values,\n",
    "        \"mean_30_2017\": get_timespan(df_2017, t2017, 30, 30).mean(axis=1).values,\n",
    "        \"mean_60_2017\": get_timespan(df_2017, t2017, 60, 60).mean(axis=1).values,\n",
    "        \"mean_140_2017\": get_timespan(df_2017, t2017, 140, 140).mean(axis=1).values,\n",
    "        \"promo_14_2017\": get_timespan(promo_2017, t2017, 14, 14).sum(axis=1).values,\n",
    "        \"promo_60_2017\": get_timespan(promo_2017, t2017, 60, 60).sum(axis=1).values,\n",
    "        \"promo_140_2017\": get_timespan(promo_2017, t2017, 140, 140).sum(axis=1).values\n",
    "    })\n",
    "    for i in range(7):\n",
    "        X['mean_4_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n",
    "        X['mean_20_dow{}_2017'.format(i)] = get_timespan(df_2017, t2017, 140-i, 20, freq='7D').mean(axis=1).values\n",
    "    for i in range(16):\n",
    "        X[\"promo_{}\".format(i)] = promo_2017[\n",
    "            t2017 + timedelta(days=i)].values.astype(np.uint8)\n",
    "    if is_train:\n",
    "        y = df_2017[\n",
    "            pd.date_range(t2017, periods=16)\n",
    "        ].values\n",
    "        return X, y\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing dataset...\")\n",
    "t2017 = date(2017, 5, 31)\n",
    "X_l, y_l = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6):\n",
    "    delta = timedelta(days=7 * i)\n",
    "    \n",
    "    X_tmp, y_tmp = prepare_dataset(\n",
    "        t2017 + delta\n",
    "    )\n",
    "    \n",
    "    X_tmp = pd.concat([X_tmp, items.reset_index(), stores.reset_index()], axis=1)\n",
    "    \n",
    "    X_l.append(X_tmp)\n",
    "    y_l.append(y_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat(X_l, axis=0)\n",
    "y_train = np.concatenate(y_l, axis=0)\n",
    "del X_l, y_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val = prepare_dataset(date(2017, 7, 26))\n",
    "X_val = pd.concat([X_val, items.reset_index(), stores.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Categorical Embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dnn = copy.copy(X_train.iloc[:,[41,42,45,46,47,48]]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_dnn = copy.copy(X_val.iloc[:,[41,42,45,46,47,48]]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "les = []\n",
    "for i in range(X_train_dnn.shape[1]):\n",
    "    le = LabelEncoder()\n",
    "    le.fit(X_train.iloc[:,[41,42,45,46,47,48]].iloc[:, i])\n",
    "    les.append(le)\n",
    "    X_train_dnn[:, i] = le.transform(X_train_dnn[:, i])\n",
    "    X_val_dnn[:, i] = le.transform(X_val_dnn[:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features(X):\n",
    "    X_list = []\n",
    "    \n",
    "    family = X[..., [0]]\n",
    "    X_list.append(family)\n",
    "    \n",
    "    class1 = X[..., [1]]\n",
    "    X_list.append(class1)\n",
    "    \n",
    "    city = X[..., [2]]\n",
    "    X_list.append(city)\n",
    "    \n",
    "    state = X[..., [3]]\n",
    "    X_list.append(state)\n",
    "    \n",
    "    type1 = X[..., [4]]\n",
    "    X_list.append(type1)\n",
    "    \n",
    "    cluster = X[..., [5]]\n",
    "    X_list.append(cluster)\n",
    "    \n",
    "    return X_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Entity Embedding essentially invoves DNN modelling with output features by looking at the target variable \n",
    "#This function NN_with_EntityEmbedding does the same.\n",
    "\n",
    "class NN_with_EntityEmbedding(object):\n",
    "    def __init__(self, X_train, y_train, X_val, y_val):\n",
    "            self.nb_epoch = 10\n",
    "            self.__build_keras_model()\n",
    "            self.fit(X_train, y_train, X_val, y_val)\n",
    "    def preprocessing(self, X):\n",
    "            X_list = split_features(X)\n",
    "            return X_list\n",
    "    def __build_keras_model(self):\n",
    "            models = []\n",
    "            \n",
    "            modelfamily_in = Input(shape=(1,))\n",
    "            modelfamily_out = Embedding(len(les[0].classes_), 16, input_length=1)(modelfamily_in)\n",
    "            modelfamily_out = Reshape(target_shape=(16,))(modelfamily_out)\n",
    "            #modelfamily = Model(modelfamily_in, modelfamily_out)\n",
    "            models.append(modelfamily_out)\n",
    "            \n",
    "            modelclass_in = Input(shape=(1,))\n",
    "            modelclass_out = Embedding(len(les[1].classes_), 50, input_length=1)(modelclass_in)\n",
    "            modelclass_out = Reshape(target_shape=(50,))(modelclass_out)\n",
    "            #modelclass = Model(modelclass_in, modelclass_out)\n",
    "            models.append(modelclass_out)\n",
    "\n",
    "            modelcity_in = Input(shape=(1,))\n",
    "            modelcity_out = Embedding(len(les[2].classes_), 11, input_length=1)(modelcity_in)\n",
    "            modelcity_out = Reshape(target_shape=(11,))(modelcity_out)\n",
    "            #modelcity = Model(modelcity_in, modelcity_out)\n",
    "            models.append(modelcity_out)\n",
    "            \n",
    "            modelstate_in = Input(shape=(1,))\n",
    "            modelstate_out = Embedding(len(les[3].classes_), 8, input_length=1)(modelstate_in)\n",
    "            modelstate_out = Reshape(target_shape=(8,))(modelstate_out)\n",
    "            #modelstate = Model(modelstate_in, modelstate_out)\n",
    "            models.append(modelstate_out)\n",
    "            \n",
    "            modeltype_in = Input(shape=(1,))\n",
    "            modeltype_out = Embedding(len(les[4].classes_), 5, input_length=1)(modeltype_in)\n",
    "            modeltype_out = Reshape(target_shape=(5,))(modeltype_out)\n",
    "            #modeltype = Model(modeltype_in, modeltype_out)\n",
    "            models.append(modeltype_out)\n",
    "            \n",
    "            modelcluster_in = Input(shape=(1,))\n",
    "            modelcluster_out = Embedding(len(les[5].classes_), 8, input_length=1)(modelcluster_in)\n",
    "            modelcluster_out = Reshape(target_shape=(8,))(modelcluster_out)\n",
    "            #modelcluster = Model(modelcluster_in, modelcluster_out)\n",
    "            models.append(modelcluster_out)\n",
    "            \n",
    "            concatenated = concatenate(models)\n",
    "            out = Dense(150, activation='relu', kernel_initializer='uniform')(concatenated)\n",
    "            out1 = Dense(250, activation='relu', kernel_initializer='uniform')(out)\n",
    "            out2 = Dense(1, activation='relu', kernel_initializer='uniform')(out1)\n",
    "            \n",
    "            self.model = Model([modelfamily_in,modelclass_in,modelcity_in,modelstate_in,\n",
    "                               modeltype_in, modelcluster_in], out2)\n",
    "            \n",
    "            self.model.compile(loss='mse', optimizer='adam', \n",
    "                                 metrics=['accuracy'])\n",
    "\n",
    "        \n",
    "    def fit(self, X_train, y_train, X_val, y_val):\n",
    "            self.model.fit(self.preprocessing(X_train), y_train,\n",
    "                           validation_data=(self.preprocessing(X_val), y_val),\n",
    "                           epochs=self.nb_epoch, batch_size=128,\n",
    "                           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/10\n",
      "1005090/1005090 [==============================] - 42s - loss: 0.7850 - acc: 0.1204 - val_loss: 0.7863 - val_acc: 0.1277\n",
      "Epoch 2/10\n",
      "1005090/1005090 [==============================] - 37s - loss: 0.7692 - acc: 0.1253 - val_loss: 0.7833 - val_acc: 0.1215\n",
      "Epoch 3/10\n",
      "1005090/1005090 [==============================] - 37s - loss: 0.7650 - acc: 0.1266 - val_loss: 0.7816 - val_acc: 0.1215\n",
      "Epoch 4/10\n",
      "1005090/1005090 [==============================] - 38s - loss: 0.7628 - acc: 0.1276 - val_loss: 0.7770 - val_acc: 0.1241\n",
      "Epoch 5/10\n",
      "1005090/1005090 [==============================] - 38s - loss: 0.7610 - acc: 0.1295 - val_loss: 0.7791 - val_acc: 0.1257\n",
      "Epoch 6/10\n",
      "1005090/1005090 [==============================] - 39s - loss: 0.7598 - acc: 0.1300 - val_loss: 0.7792 - val_acc: 0.1229\n",
      "Epoch 7/10\n",
      "1005090/1005090 [==============================] - 37s - loss: 0.7587 - acc: 0.1299 - val_loss: 0.7759 - val_acc: 0.1281\n",
      "Epoch 8/10\n",
      "1005090/1005090 [==============================] - 41s - loss: 0.7577 - acc: 0.1309 - val_loss: 0.7780 - val_acc: 0.1288\n",
      "Epoch 9/10\n",
      "1005090/1005090 [==============================] - 41s - loss: 0.7570 - acc: 0.1312 - val_loss: 0.7741 - val_acc: 0.1343\n",
      "Epoch 10/10\n",
      "1005090/1005090 [==============================] - 40s - loss: 0.7565 - acc: 0.1309 - val_loss: 0.7755 - val_acc: 0.1266\n"
     ]
    }
   ],
   "source": [
    "dnn = NN_with_EntityEmbedding(X_train_dnn, y_train[:,[15]], X_val_dnn, y_val[:,[15]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = dnn.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting embedding weight for 'item family'\n",
    "item_family_embeddings = pd.DataFrame(weights[0])\n",
    "item_family_embeddings.columns = ['family'+str(i) for i in range(len(item_family_embeddings.columns.values))]\n",
    "item_family_embeddings['family'] = les[0].inverse_transform(np.unique(X_train_dnn[..., [0]]))\n",
    "item_family_embeddings = item_family_embeddings.set_index('family')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting embedding weight for 'item class'\n",
    "item_class_embeddings = pd.DataFrame(weights[1])\n",
    "item_class_embeddings.columns = ['class'+str(i) for i in range(len(item_class_embeddings.columns.values))]\n",
    "item_class_embeddings['class'] = les[1].inverse_transform(np.unique(X_train_dnn[..., [1]]))\n",
    "item_class_embeddings = item_class_embeddings.set_index('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting embedding weight for 'store city'\n",
    "store_city_embeddings = pd.DataFrame(weights[2])\n",
    "store_city_embeddings.columns = ['city'+str(i) for i in range(len(store_city_embeddings.columns.values))]\n",
    "store_city_embeddings['city'] = les[2].inverse_transform(np.unique(X_train_dnn[..., [2]]))\n",
    "store_city_embeddings = store_city_embeddings.set_index('city')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting embedding weight for 'store state'\n",
    "store_state_embeddings = pd.DataFrame(weights[3])\n",
    "store_state_embeddings.columns = ['state'+str(i) for i in range(len(store_state_embeddings.columns.values))]\n",
    "store_state_embeddings['state'] = les[3].inverse_transform(np.unique(X_train_dnn[..., [3]]))\n",
    "store_state_embeddings = store_state_embeddings.set_index('state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting embedding weight for 'store type'\n",
    "store_type_embeddings = pd.DataFrame(weights[4])\n",
    "store_type_embeddings.columns = ['type'+str(i) for i in range(len(store_type_embeddings.columns.values))]\n",
    "store_type_embeddings['type'] = les[4].inverse_transform(np.unique(X_train_dnn[..., [4]]))\n",
    "store_type_embeddings = store_type_embeddings.set_index('type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting embedding weight for 'store cluster'\n",
    "store_cluster_embeddings = pd.DataFrame(weights[5])\n",
    "store_cluster_embeddings.columns = ['cluster'+str(i) for i in range(len(store_cluster_embeddings.columns.values))]\n",
    "store_cluster_embeddings['cluster'] = les[5].inverse_transform(np.unique(X_train_dnn[..., [5]]))\n",
    "store_cluster_embeddings = store_cluster_embeddings.set_index('cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural net model with Categorical Embedding Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding embedding as features onto Xtrain array\n",
    "X_train_1 = X_train.join(item_family_embeddings, on = 'family', how = \"left\")\n",
    "X_train_1 = X_train_1.join(item_class_embeddings, on = 'class', how = \"left\")\n",
    "X_train_1 = X_train_1.join(store_city_embeddings, on = 'city', how = \"left\")\n",
    "X_train_1 = X_train_1.join(store_state_embeddings, on = 'state', how = \"left\")\n",
    "X_train_1 = X_train_1.join(store_type_embeddings, on = 'type', how = \"left\")\n",
    "X_train_1 = X_train_1.join(store_cluster_embeddings, on = 'cluster', how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding embedding as features onto Xval array\n",
    "X_val_1 = X_val.join(item_family_embeddings, on = 'family', how = \"left\")\n",
    "X_val_1 = X_val_1.join(item_class_embeddings, on = 'class', how = \"left\")\n",
    "X_val_1 = X_val_1.join(store_city_embeddings, on = 'city', how = \"left\")\n",
    "X_val_1 = X_val_1.join(store_state_embeddings, on = 'state', how = \"left\")\n",
    "X_val_1 = X_val_1.join(store_type_embeddings, on = 'type', how = \"left\")\n",
    "X_val_1 = X_val_1.join(store_cluster_embeddings, on = 'cluster', how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = prepare_dataset(date(2017, 8, 16), is_train=False)\n",
    "X_test = pd.concat([X_test, items.reset_index(), stores.reset_index()], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding embedding as features onto Xtest array\n",
    "X_test_1 = X_test.join(item_family_embeddings, on = 'family', how = \"left\")\n",
    "X_test_1 = X_test_1.join(item_class_embeddings, on = 'class', how = \"left\")\n",
    "X_test_1 = X_test_1.join(store_city_embeddings, on = 'city', how = \"left\")\n",
    "X_test_1 = X_test_1.join(store_state_embeddings, on = 'state', how = \"left\")\n",
    "X_test_1 = X_test_1.join(store_type_embeddings, on = 'type', how = \"left\")\n",
    "X_test_1 = X_test_1.join(store_cluster_embeddings, on = 'cluster', how = \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1005090, 49)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop original item and store categoricals as they are now replaced with entity embedding\n",
    "cols = [40,41,42,43,44,45,46,47,48]\n",
    "X_train_1.drop(X_train_1.columns[cols],axis=1,inplace=True)\n",
    "X_val_1.drop(X_val_1.columns[cols],axis=1,inplace=True)\n",
    "X_test_1.drop(X_test_1.columns[cols],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores_items = pd.DataFrame(index=df_2017.index)\n",
    "test_ids = df_test[['id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = items.reindex( stores_items.index.get_level_values(1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.as_matrix()\n",
    "X_test = X_test.as_matrix()\n",
    "X_val = X_val.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = X_train_1.as_matrix()\n",
    "X_test_1 = X_test_1.as_matrix()\n",
    "X_val_1 = X_val_1.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1 = X_train_1.reshape((X_train_1.shape[0], 1, X_train_1.shape[1]))\n",
    "X_test_1 = X_test_1.reshape((X_test_1.shape[0], 1, X_test_1.shape[1]))\n",
    "X_val_1 = X_val_1.reshape((X_val_1.shape[0], 1, X_val_1.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras model Defintion\n",
    "model = Sequential()\n",
    "model.add(LSTM(512, input_shape=(X_train_1.shape[1],X_train_1.shape[2])))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.1))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.1))\n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.05))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.05))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.05))\n",
    "\n",
    "model.add(Dense(16))\n",
    "model.add(PReLU())\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.05))\n",
    "\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss = 'mse', optimizer='adam', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Step 1\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "63s - loss: 0.4585 - mean_squared_error: 0.4333 - val_loss: 0.2993 - val_mean_squared_error: 0.2993\n",
      "Epoch 2/5\n",
      "60s - loss: 0.3439 - mean_squared_error: 0.3260 - val_loss: 0.3066 - val_mean_squared_error: 0.3066\n",
      "Epoch 3/5\n",
      "60s - loss: 0.3355 - mean_squared_error: 0.3183 - val_loss: 0.2935 - val_mean_squared_error: 0.2935\n",
      "Epoch 4/5\n",
      "60s - loss: 0.3321 - mean_squared_error: 0.3152 - val_loss: 0.3013 - val_mean_squared_error: 0.3013\n",
      "Epoch 5/5\n",
      "61s - loss: 0.3290 - mean_squared_error: 0.3124 - val_loss: 0.2925 - val_mean_squared_error: 0.2925\n",
      "==================================================\n",
      "Step 2\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "60s - loss: 0.3598 - mean_squared_error: 0.3395 - val_loss: 0.3322 - val_mean_squared_error: 0.3322\n",
      "Epoch 2/5\n",
      "61s - loss: 0.3542 - mean_squared_error: 0.3343 - val_loss: 0.3266 - val_mean_squared_error: 0.3266\n",
      "Epoch 3/5\n",
      "63s - loss: 0.3521 - mean_squared_error: 0.3324 - val_loss: 0.3241 - val_mean_squared_error: 0.3241\n",
      "Epoch 4/5\n",
      "61s - loss: 0.3507 - mean_squared_error: 0.3312 - val_loss: 0.3240 - val_mean_squared_error: 0.3240\n",
      "Epoch 5/5\n",
      "63s - loss: 0.3490 - mean_squared_error: 0.3296 - val_loss: 0.3255 - val_mean_squared_error: 0.3255\n",
      "==================================================\n",
      "Step 3\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "60s - loss: 0.3567 - mean_squared_error: 0.3380 - val_loss: 0.3361 - val_mean_squared_error: 0.3361\n",
      "Epoch 2/5\n",
      "63s - loss: 0.3489 - mean_squared_error: 0.3310 - val_loss: 0.3405 - val_mean_squared_error: 0.3405\n",
      "Epoch 3/5\n",
      "62s - loss: 0.3466 - mean_squared_error: 0.3289 - val_loss: 0.3364 - val_mean_squared_error: 0.3364\n",
      "Epoch 4/5\n",
      "62s - loss: 0.3452 - mean_squared_error: 0.3276 - val_loss: 0.3404 - val_mean_squared_error: 0.3404\n",
      "Epoch 5/5\n",
      "62s - loss: 0.3437 - mean_squared_error: 0.3262 - val_loss: 0.3358 - val_mean_squared_error: 0.3358\n",
      "==================================================\n",
      "Step 4\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "61s - loss: 0.3811 - mean_squared_error: 0.3605 - val_loss: 0.3528 - val_mean_squared_error: 0.3528\n",
      "Epoch 2/5\n",
      "62s - loss: 0.3722 - mean_squared_error: 0.3522 - val_loss: 0.3500 - val_mean_squared_error: 0.3500\n",
      "Epoch 3/5\n",
      "62s - loss: 0.3696 - mean_squared_error: 0.3499 - val_loss: 0.3503 - val_mean_squared_error: 0.3503\n",
      "Epoch 4/5\n",
      "61s - loss: 0.3676 - mean_squared_error: 0.3479 - val_loss: 0.3484 - val_mean_squared_error: 0.3484\n",
      "Epoch 5/5\n",
      "62s - loss: 0.3661 - mean_squared_error: 0.3465 - val_loss: 0.3520 - val_mean_squared_error: 0.3520\n",
      "==================================================\n",
      "Step 5\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "60s - loss: 0.3886 - mean_squared_error: 0.3679 - val_loss: 0.3522 - val_mean_squared_error: 0.3522\n",
      "Epoch 2/5\n",
      "62s - loss: 0.3789 - mean_squared_error: 0.3589 - val_loss: 0.3508 - val_mean_squared_error: 0.3508\n",
      "Epoch 3/5\n",
      "62s - loss: 0.3754 - mean_squared_error: 0.3556 - val_loss: 0.3510 - val_mean_squared_error: 0.3510\n",
      "Epoch 4/5\n",
      "63s - loss: 0.3733 - mean_squared_error: 0.3537 - val_loss: 0.3552 - val_mean_squared_error: 0.3552\n",
      "Epoch 5/5\n",
      "61s - loss: 0.3719 - mean_squared_error: 0.3524 - val_loss: 0.3538 - val_mean_squared_error: 0.3538\n",
      "==================================================\n",
      "Step 6\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "61s - loss: 0.3828 - mean_squared_error: 0.3610 - val_loss: 0.3591 - val_mean_squared_error: 0.3591\n",
      "Epoch 2/5\n",
      "63s - loss: 0.3730 - mean_squared_error: 0.3520 - val_loss: 0.3682 - val_mean_squared_error: 0.3682\n",
      "Epoch 3/5\n",
      "62s - loss: 0.3708 - mean_squared_error: 0.3499 - val_loss: 0.3631 - val_mean_squared_error: 0.3631\n",
      "Epoch 4/5\n",
      "63s - loss: 0.3693 - mean_squared_error: 0.3485 - val_loss: 0.3618 - val_mean_squared_error: 0.3618\n",
      "Epoch 5/5\n",
      "61s - loss: 0.3678 - mean_squared_error: 0.3473 - val_loss: 0.3699 - val_mean_squared_error: 0.3699\n",
      "==================================================\n",
      "Step 7\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "62s - loss: 0.3679 - mean_squared_error: 0.3480 - val_loss: 0.4404 - val_mean_squared_error: 0.4404\n",
      "Epoch 2/5\n",
      "62s - loss: 0.3601 - mean_squared_error: 0.3408 - val_loss: 0.4357 - val_mean_squared_error: 0.4357\n",
      "Epoch 3/5\n",
      "62s - loss: 0.3576 - mean_squared_error: 0.3386 - val_loss: 0.4331 - val_mean_squared_error: 0.4331\n",
      "Epoch 4/5\n",
      "63s - loss: 0.3564 - mean_squared_error: 0.3374 - val_loss: 0.4528 - val_mean_squared_error: 0.4528\n",
      "Epoch 5/5\n",
      "63s - loss: 0.3555 - mean_squared_error: 0.3366 - val_loss: 0.4428 - val_mean_squared_error: 0.4428\n",
      "==================================================\n",
      "Step 8\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "61s - loss: 0.3536 - mean_squared_error: 0.3360 - val_loss: 0.3996 - val_mean_squared_error: 0.3996\n",
      "Epoch 2/5\n",
      "63s - loss: 0.3441 - mean_squared_error: 0.3275 - val_loss: 0.4020 - val_mean_squared_error: 0.4020\n",
      "Epoch 3/5\n",
      "62s - loss: 0.3417 - mean_squared_error: 0.3253 - val_loss: 0.4115 - val_mean_squared_error: 0.4115\n",
      "Epoch 4/5\n",
      "63s - loss: 0.3399 - mean_squared_error: 0.3236 - val_loss: 0.3915 - val_mean_squared_error: 0.3915\n",
      "Epoch 5/5\n",
      "62s - loss: 0.3386 - mean_squared_error: 0.3224 - val_loss: 0.3985 - val_mean_squared_error: 0.3985\n",
      "==================================================\n",
      "Step 9\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "61s - loss: 0.3651 - mean_squared_error: 0.3449 - val_loss: 0.4063 - val_mean_squared_error: 0.4063\n",
      "Epoch 2/5\n",
      "63s - loss: 0.3558 - mean_squared_error: 0.3363 - val_loss: 0.3787 - val_mean_squared_error: 0.3787\n",
      "Epoch 3/5\n",
      "61s - loss: 0.3537 - mean_squared_error: 0.3344 - val_loss: 0.3939 - val_mean_squared_error: 0.3939\n",
      "Epoch 4/5\n",
      "62s - loss: 0.3516 - mean_squared_error: 0.3324 - val_loss: 0.3982 - val_mean_squared_error: 0.3982\n",
      "Epoch 5/5\n",
      "62s - loss: 0.3499 - mean_squared_error: 0.3309 - val_loss: 0.3877 - val_mean_squared_error: 0.3877\n",
      "==================================================\n",
      "Step 10\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "61s - loss: 0.3652 - mean_squared_error: 0.3463 - val_loss: 0.3793 - val_mean_squared_error: 0.3793\n",
      "Epoch 2/5\n",
      "63s - loss: 0.3541 - mean_squared_error: 0.3363 - val_loss: 0.3752 - val_mean_squared_error: 0.3752\n",
      "Epoch 3/5\n",
      "63s - loss: 0.3506 - mean_squared_error: 0.3331 - val_loss: 0.3869 - val_mean_squared_error: 0.3869\n",
      "Epoch 4/5\n",
      "62s - loss: 0.3488 - mean_squared_error: 0.3314 - val_loss: 0.3811 - val_mean_squared_error: 0.3811\n",
      "Epoch 5/5\n",
      "63s - loss: 0.3472 - mean_squared_error: 0.3300 - val_loss: 0.3814 - val_mean_squared_error: 0.3814\n",
      "==================================================\n",
      "Step 11\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "64s - loss: 0.3923 - mean_squared_error: 0.3716 - val_loss: 0.3869 - val_mean_squared_error: 0.3869\n",
      "Epoch 2/5\n",
      "61s - loss: 0.3805 - mean_squared_error: 0.3606 - val_loss: 0.3876 - val_mean_squared_error: 0.3876\n",
      "Epoch 3/5\n",
      "61s - loss: 0.3771 - mean_squared_error: 0.3573 - val_loss: 0.3973 - val_mean_squared_error: 0.3973\n",
      "Epoch 4/5\n",
      "61s - loss: 0.3750 - mean_squared_error: 0.3554 - val_loss: 0.3856 - val_mean_squared_error: 0.3856\n",
      "Epoch 5/5\n",
      "60s - loss: 0.3732 - mean_squared_error: 0.3538 - val_loss: 0.3784 - val_mean_squared_error: 0.3784\n",
      "==================================================\n",
      "Step 12\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "60s - loss: 0.3983 - mean_squared_error: 0.3775 - val_loss: 0.3909 - val_mean_squared_error: 0.3909\n",
      "Epoch 2/5\n",
      "60s - loss: 0.3856 - mean_squared_error: 0.3656 - val_loss: 0.4006 - val_mean_squared_error: 0.4006\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60s - loss: 0.3815 - mean_squared_error: 0.3619 - val_loss: 0.3960 - val_mean_squared_error: 0.3960\n",
      "Epoch 4/5\n",
      "61s - loss: 0.3791 - mean_squared_error: 0.3596 - val_loss: 0.3939 - val_mean_squared_error: 0.3939\n",
      "Epoch 5/5\n",
      "60s - loss: 0.3774 - mean_squared_error: 0.3580 - val_loss: 0.3971 - val_mean_squared_error: 0.3971\n",
      "==================================================\n",
      "Step 13\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "61s - loss: 0.3890 - mean_squared_error: 0.3672 - val_loss: 0.3924 - val_mean_squared_error: 0.3924\n",
      "Epoch 2/5\n",
      "60s - loss: 0.3769 - mean_squared_error: 0.3560 - val_loss: 0.3900 - val_mean_squared_error: 0.3900\n",
      "Epoch 3/5\n",
      "60s - loss: 0.3740 - mean_squared_error: 0.3533 - val_loss: 0.3918 - val_mean_squared_error: 0.3918\n",
      "Epoch 4/5\n",
      "61s - loss: 0.3722 - mean_squared_error: 0.3516 - val_loss: 0.3925 - val_mean_squared_error: 0.3925\n",
      "Epoch 5/5\n",
      "61s - loss: 0.3706 - mean_squared_error: 0.3501 - val_loss: 0.3920 - val_mean_squared_error: 0.3920\n",
      "==================================================\n",
      "Step 14\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "60s - loss: 0.3745 - mean_squared_error: 0.3543 - val_loss: 0.3803 - val_mean_squared_error: 0.3803\n",
      "Epoch 2/5\n",
      "60s - loss: 0.3641 - mean_squared_error: 0.3447 - val_loss: 0.3801 - val_mean_squared_error: 0.3801\n",
      "Epoch 3/5\n",
      "60s - loss: 0.3618 - mean_squared_error: 0.3426 - val_loss: 0.3796 - val_mean_squared_error: 0.3796\n",
      "Epoch 4/5\n",
      "61s - loss: 0.3601 - mean_squared_error: 0.3410 - val_loss: 0.3809 - val_mean_squared_error: 0.3809\n",
      "Epoch 5/5\n",
      "61s - loss: 0.3587 - mean_squared_error: 0.3398 - val_loss: 0.3769 - val_mean_squared_error: 0.3769\n",
      "==================================================\n",
      "Step 15\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "61s - loss: 0.3603 - mean_squared_error: 0.3424 - val_loss: 0.3676 - val_mean_squared_error: 0.3676\n",
      "Epoch 2/5\n",
      "60s - loss: 0.3491 - mean_squared_error: 0.3323 - val_loss: 0.3663 - val_mean_squared_error: 0.3663\n",
      "Epoch 3/5\n",
      "60s - loss: 0.3466 - mean_squared_error: 0.3300 - val_loss: 0.3678 - val_mean_squared_error: 0.3678\n",
      "Epoch 4/5\n",
      "61s - loss: 0.3448 - mean_squared_error: 0.3283 - val_loss: 0.3677 - val_mean_squared_error: 0.3677\n",
      "Epoch 5/5\n",
      "60s - loss: 0.3430 - mean_squared_error: 0.3266 - val_loss: 0.3656 - val_mean_squared_error: 0.3656\n",
      "==================================================\n",
      "Step 16\n",
      "==================================================\n",
      "Train on 1005090 samples, validate on 167515 samples\n",
      "Epoch 1/5\n",
      "61s - loss: 0.3728 - mean_squared_error: 0.3523 - val_loss: 0.3906 - val_mean_squared_error: 0.3906\n",
      "Epoch 2/5\n",
      "60s - loss: 0.3631 - mean_squared_error: 0.3432 - val_loss: 0.3855 - val_mean_squared_error: 0.3855\n",
      "Epoch 3/5\n",
      "60s - loss: 0.3602 - mean_squared_error: 0.3405 - val_loss: 0.3860 - val_mean_squared_error: 0.3860\n",
      "Epoch 4/5\n",
      "61s - loss: 0.3582 - mean_squared_error: 0.3387 - val_loss: 0.3907 - val_mean_squared_error: 0.3907\n",
      "Epoch 5/5\n",
      "60s - loss: 0.3566 - mean_squared_error: 0.3372 - val_loss: 0.3919 - val_mean_squared_error: 0.3919\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "\n",
    "val_pred = []\n",
    "test_pred = []\n",
    "# wtpath = 'weights.hdf5'  # To save best epoch. But need Keras bug to be fixed first.\n",
    "sample_weights=np.array( pd.concat([items[\"perishable\"]] * 6) * 0.25 + 1 )\n",
    "for i in range(16):\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Step %d\" % (i+1))\n",
    "    print(\"=\" * 50)\n",
    "    y = y_train[:, i]\n",
    "    xv = X_val_1\n",
    "    yv = y_val[:, i]\n",
    "    model.fit(X_train_1, y, batch_size = 512, epochs = N_EPOCHS, verbose=2,\n",
    "               sample_weight=sample_weights, validation_data=(xv,yv) ) \n",
    "    val_pred.append(model.predict(X_val_1))\n",
    "    test_pred.append(model.predict(X_test_1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unweighted validation mse:  0.371372460586\n",
      "Full validation mse:        0.370393038575\n",
      "'Public' validation mse:    0.332081564279\n",
      "'Private' validation mse:   0.387807345073\n"
     ]
    }
   ],
   "source": [
    "n_public = 5 # Number of days in public test set\n",
    "weights=pd.concat([items[\"perishable\"]]) * 0.25 + 1\n",
    "print(\"Unweighted validation mse: \", mean_squared_error(\n",
    "    y_val, np.array(val_pred).squeeze(axis=2).transpose()) )\n",
    "print(\"Full validation mse:       \", mean_squared_error(\n",
    "    y_val, np.array(val_pred).squeeze(axis=2).transpose(), sample_weight=weights) )\n",
    "print(\"'Public' validation mse:   \", mean_squared_error(\n",
    "    y_val[:,:n_public], np.array(val_pred).squeeze(axis=2).transpose()[:,:n_public], \n",
    "    sample_weight=weights) )\n",
    "print(\"'Private' validation mse:  \", mean_squared_error(\n",
    "    y_val[:,n_public:], np.array(val_pred).squeeze(axis=2).transpose()[:,n_public:], \n",
    "    sample_weight=weights) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(test_pred).squeeze(axis=2).transpose()\n",
    "df_preds = pd.DataFrame(\n",
    "    y_test, index=stores_items.index,\n",
    "    columns=pd.date_range(\"2017-08-16\", periods=16)\n",
    ").stack().to_frame(\"unit_sales\")\n",
    "df_preds.index.set_names([\"store_nbr\", \"item_nbr\", \"date\"], inplace=True)\n",
    "\n",
    "submission = test_ids.join(df_preds, how=\"left\").fillna(0)\n",
    "submission[\"unit_sales\"] = np.clip(np.expm1(submission[\"unit_sales\"]), 0, 1000)\n",
    "submission.to_csv('lstm_w_cat_em.csv', float_format='%.4f', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
